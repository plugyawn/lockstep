#!/usr/bin/env python3
"""
Test script to load and use available pre-trained checkpoints for DUO and D2F models.
"""

import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import time
from pathlib import Path


def test_duo_checkpoint():
    """Test DUO pre-trained checkpoints from HuggingFace"""

    print("=" * 80)
    print("Testing DUO Checkpoints")
    print("=" * 80)

    try:
        # Load DUO distilled model
        print("\n1. Loading DUO distilled model...")
        model_name = "s-sahoo/duo-distilled"

        print(f"   Loading from: {model_name}")
        model = AutoModelForMaskedLM.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained("gpt2")

        print(f"   âœ“ Model loaded successfully")
        print(f"   Model size: {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters")

        # Test generation
        print("\n2. Testing generation with DUO-distilled...")
        prompt = "The key to understanding diffusion models is"
        inputs = tokenizer(prompt, return_tensors="pt", padding=True)

        # Mask some tokens for prediction
        input_ids = inputs.input_ids
        # Create mask tokens
        mask_token_id = tokenizer.mask_token_id if tokenizer.mask_token_id else tokenizer.vocab_size

        print(f"   Input: {prompt}")

        with torch.no_grad():
            start = time.time()
            outputs = model(**inputs)
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)
            elapsed = time.time() - start

        print(f"   âœ“ Generation completed in {elapsed:.3f}s")
        print(f"   Output shape: {logits.shape}")

        # Try loading un-distilled version
        print("\n3. Loading DUO un-distilled model...")
        model_name_undistilled = "s-sahoo/duo"

        try:
            model_undistilled = AutoModelForMaskedLM.from_pretrained(model_name_undistilled)
            print(f"   âœ“ Un-distilled model loaded successfully")
        except Exception as e:
            print(f"   âš  Could not load un-distilled model: {e}")

    except Exception as e:
        print(f"   âœ— Error loading DUO model: {e}")
        print("   Note: You may need to install additional dependencies or authenticate with HuggingFace")


def test_d2f_checkpoint():
    """Test D2F LoRA checkpoints from HuggingFace"""

    print("\n" + "=" * 80)
    print("Testing D2F Checkpoints")
    print("=" * 80)

    try:
        # Load D2F Dream-7B LoRA
        print("\n1. Loading D2F-Dream-7B-LoRA...")
        lora_name = "SJTU-Deng-Lab/D2F_Dream_Base_7B_Lora"
        base_model_name = "kuleshov-group/dream-7b"

        print(f"   Loading base model: {base_model_name}")
        # Note: This would require the actual Dream-7B model
        # For demonstration, we'll show the loading pattern

        print(f"   Would load LoRA from: {lora_name}")
        print("   Note: Requires Dream-7B base model and PEFT library")

        # Simulated loading (would need actual model)
        """
        base_model = AutoModelForCausalLM.from_pretrained(base_model_name)
        d2f_model = PeftModel.from_pretrained(base_model, lora_name)
        print(f"   âœ“ D2F LoRA loaded successfully")
        """

        print("\n2. D2F-LLaDA-Instruct-8B-LoRA info:")
        print(f"   Model: SJTU-Deng-Lab/D2F_LLaDA_Instruct_8B_Lora")
        print(f"   Type: Instruction-tuned 8B model with LoRA")
        print(f"   Speedup: 50Ã— vs vanilla diffusion LMs")
        print(f"   Demo: https://huggingface.co/spaces/zhijie3/D2F-LLaDA-Instruct-8B")

    except Exception as e:
        print(f"   âœ— Error with D2F models: {e}")
        print("   Note: D2F models require base Dream or LLaDA models")


def compare_checkpoints():
    """Compare available checkpoints for lockstep R/A generation"""

    print("\n" + "=" * 80)
    print("Checkpoint Comparison for Lockstep R/A")
    print("=" * 80)

    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Model                   â”‚ Ready to Use â”‚ Speedup â”‚ Best Use Case    â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ s-sahoo/duo-distilled   â”‚ âœ“ Yes        â”‚ 100Ã—    â”‚ Quick testing    â”‚")
    print("â”‚ s-sahoo/duo             â”‚ âœ“ Yes        â”‚ 10Ã—     â”‚ Quality focus    â”‚")
    print("â”‚ D2F-Dream-7B-LoRA       â”‚ Needs Dream  â”‚ 50Ã—     â”‚ Dream compatible â”‚")
    print("â”‚ D2F-LLaDA-8B-LoRA       â”‚ Needs LLaDA  â”‚ 50Ã—     â”‚ Instruction tasksâ”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

    print("\nğŸ“Š Summary:")
    print("â€¢ DUO models are standalone and ready to use")
    print("â€¢ D2F models are LoRA adaptations requiring base models")
    print("â€¢ DUO-distilled offers best speedup (100Ã—) with 8-step generation")
    print("â€¢ D2F provides KV cache support for production deployment")


def create_integration_script():
    """Create a script to integrate checkpoints with lockstep controller"""

    script_content = '''#!/usr/bin/env python3
"""
Integration script for using pre-trained DUO/D2F checkpoints with lockstep R/A generation.
"""

import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer
from src.core.lockstep_controller import LockstepController
from omegaconf import DictConfig


class PretrainedLockstepGenerator:
    """Integrate pre-trained checkpoints with lockstep R/A control"""

    def __init__(self, model_name="s-sahoo/duo-distilled"):
        # Load pre-trained model
        self.model = AutoModelForMaskedLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained("gpt2")

        # Setup lockstep controller
        cfg = DictConfig({
            'decode': {
                'lockstep': {
                    'mode': 'gauss_seidel',
                    'r_span': [20, 40],
                    'a_span': [40, 50],
                    'tau_r': 0.9,
                    'tau_a': 0.85,
                    'fill_rate_cap': 0.2,
                    'coupling_strength': 0.1
                }
            }
        })
        self.controller = LockstepController(cfg)

    def generate_lockstep(self, prompt, max_length=128, num_steps=8):
        """Generate with lockstep R/A control using few-step DUO"""

        # Tokenize prompt
        inputs = self.tokenizer(prompt, return_tensors="pt",
                               max_length=max_length,
                               padding="max_length",
                               truncation=True)

        input_ids = inputs.input_ids
        batch_size, seq_len = input_ids.shape

        # Initialize with masked tokens
        x = input_ids.clone()
        mask_positions = torch.arange(len(prompt.split()), seq_len)
        x[0, mask_positions] = self.tokenizer.mask_token_id or self.tokenizer.vocab_size

        # Few-step generation with lockstep control
        for step in range(num_steps):
            # Get model predictions
            with torch.no_grad():
                outputs = self.model(x)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=-1)

            # Apply lockstep control
            x = self.controller.tokens_hook(
                x, logits, None, step, num_steps
            )

        # Decode result
        generated = self.tokenizer.decode(x[0], skip_special_tokens=True)
        return generated


if __name__ == "__main__":
    # Example usage
    generator = PretrainedLockstepGenerator("s-sahoo/duo-distilled")

    prompt = "To solve this complex problem, we need to"
    result = generator.generate_lockstep(prompt, num_steps=8)

    print(f"Prompt: {prompt}")
    print(f"Generated: {result}")
    print(f"\\nUsing 8-step generation (100Ã— faster than standard diffusion)")
'''

    # Save integration script
    integration_path = Path("integrate_checkpoints.py")
    integration_path.write_text(script_content)
    print(f"\nâœ“ Created integration script: {integration_path}")


def main():
    """Main test function"""

    print("\n" + "ğŸš€ " * 20)
    print("TESTING AVAILABLE CHECKPOINTS FOR LOCKSTEP R/A")
    print("ğŸš€ " * 20)

    # Test DUO checkpoints
    test_duo_checkpoint()

    # Test D2F checkpoints
    test_d2f_checkpoint()

    # Compare checkpoints
    compare_checkpoints()

    # Create integration script
    create_integration_script()

    print("\n" + "=" * 80)
    print("RECOMMENDATION FOR IMMEDIATE USE")
    print("=" * 80)
    print("\nâœ… Use s-sahoo/duo-distilled for immediate testing:")
    print("   â€¢ Pre-trained and ready to use")
    print("   â€¢ 100Ã— speedup with 8-step generation")
    print("   â€¢ No additional training required")
    print("   â€¢ Compatible with standard transformers library")
    print("\nğŸ“ Next steps:")
    print("   1. Run integrate_checkpoints.py to test with lockstep")
    print("   2. Fine-tune on your specific R/A data if needed")
    print("   3. Consider D2F if KV cache is critical for deployment")


if __name__ == "__main__":
    main()